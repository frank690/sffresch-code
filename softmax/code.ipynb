{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "245899cf",
   "metadata": {},
   "source": [
    "#### import needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c2d2344",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbbbc3d",
   "metadata": {},
   "source": [
    "#### problems\n",
    "assume big_h or small_h are the outputs of our neural network (logits).\n",
    "Simply applying the exponential function to these values, leads to problems. For the big numbers, these are directly visible in form of overflow. The very small values might lead to potential division by zero values in further computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f31a3776",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-eae131ea25ce>:2: RuntimeWarning: overflow encountered in exp\n",
      "  softmax_nominator = np.exp(big_h)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.73927494e+018,             inf, 7.98043234e+241])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_h = [42, 1337, 557]\n",
    "softmax_nominator = np.exp(big_h)\n",
    "softmax_nominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca0acb2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.74952226e-019, 0.00000000e+000, 1.25306494e-242])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_h = [-42, -1337, -557]\n",
    "softmax_nominator = np.exp(small_h)\n",
    "softmax_nominator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d7aeb7",
   "metadata": {},
   "source": [
    "#### workaround\n",
    "again assume we put in the numbers in `hypothesis` into our softmax (apply the exponential function to them, among other things). But this time we subtract each value in `hypothesis` by the biggest value. As you can see, previously unprocessable values can thus be handled with ease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b0a6a3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -2,   0, -15])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypothesis = [1335, 1337, 1322]\n",
    "safe_hypothesis = hypothesis - np.max(hypothesis)\n",
    "safe_hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01cbe72d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.35335283e-01, 1.00000000e+00, 3.05902321e-07])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nominator = np.exp(safe_hypothesis)\n",
    "nominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15eb7d7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1353355891389334"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "denominator = np.sum(nominator)\n",
    "denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b1f5c3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.19202890e-01, 8.80796841e-01, 2.69437797e-07])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = nominator / denominator\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775c3727",
   "metadata": {},
   "source": [
    "#### safe softmax as a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdba2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z: np.ndarray, gradient: bool = False) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Apply the softmax transformation to the given vector (z).\n",
    "    If the provided gradient flag is true, return the derivative of the softmax.\n",
    "    :param z: Data vector of shape (C X 1)\n",
    "    :param gradient: Boolean flag. Default is False.\n",
    "    :return: Softmax transformed values or the gradient of the softmax.\n",
    "    \"\"\"\n",
    "    C, _ = z.shape\n",
    "    \n",
    "    z -= np.max(z, axis=1, keepdims=True)  # for numeric stability\n",
    "    S = np.exp(z) / np.exp(z).sum(axis=1, keepdims=True)\n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "95e4dfa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.20283537, 0.60935113, 0.01009858, 0.02745078, 0.15026414]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypothesis = np.array([[2, 3.1, -1, 0, 1.7]])\n",
    "softmax(hypothesis)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
